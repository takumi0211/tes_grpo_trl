2025-11-02 05:41:25,743 | INFO | train_grpo | Loaded policy model dtype=torch.bfloat16
2025-11-02 05:41:25,744 | INFO | train_grpo | HF device map: {'': 0}
2025-11-02 05:41:25,899 | INFO | train_grpo | LoRA target parameters enumerated (0): all linear layers
2025-11-02 05:41:25,902 | INFO | train_grpo | Trainable LoRA parameters (192): ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight shape=(4096, 4)', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight shape=(512, 4)', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight shape=(512, 4)', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight shape=(4, 4096)', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight shape=(2880, 4)', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight shape=(4096, 4)', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight shape=(512, 4)', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight shape=(512, 4)', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight shape=(4, 4096)', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight shape=(2880, 4)', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight shape=(4096, 4)', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight shape=(512, 4)'] ... (+172 more)
2025-11-02 05:41:26,319 | INFO | train_grpo | StepStream configured | prompts_per_micro_step=1 | num_generations=4 | dataset_rows=30 | keep_keys=['reward_action_0', 'reward_action_1', 'reward_action_2', 'reward_action_3', 'prompt']
2025-11-02 05:41:26,367 | INFO | train_grpo | Generation config | num_generations=4 | generation_batch_size=4 | per_device_train_batch_size=4 | grad_accum=4 | split_batches=True | completions_per_micro_step=4 | completions_per_update=16
2025-11-02 05:41:26,396 | INFO | train_grpo | Starting training | total_steps=10 | output_dir=runs/grpo_gptoss20b_lora4_tes
2025-11-02 05:42:47,193 | INFO | train_grpo | Loaded policy model dtype=torch.bfloat16
2025-11-02 05:42:47,194 | INFO | train_grpo | HF device map: {'': 0}
2025-11-02 05:42:47,194 | INFO | train_grpo | LoRA target parameters enumerated (0): all linear layers
2025-11-02 05:42:47,196 | WARNING | train_grpo | No LoRA parameters detected as trainable.
2025-11-02 05:42:47,555 | INFO | train_grpo | StepStream configured | prompts_per_micro_step=1 | num_generations=4 | dataset_rows=30 | keep_keys=['reward_action_0', 'reward_action_1', 'reward_action_2', 'reward_action_3', 'prompt']
2025-11-02 05:42:47,606 | INFO | train_grpo | Generation config | num_generations=4 | generation_batch_size=4 | per_device_train_batch_size=4 | grad_accum=4 | split_batches=True | completions_per_micro_step=4 | completions_per_update=16
2025-11-02 05:42:47,627 | INFO | train_grpo | Starting training | total_steps=10 | output_dir=runs/grpo_gptoss20b_lora4_tes
2025-11-02 05:45:23,722 | INFO | train_grpo | Loaded policy model dtype=torch.bfloat16
2025-11-02 05:45:23,723 | INFO | train_grpo | HF device map: {'': 0}
2025-11-02 05:45:23,923 | INFO | train_grpo | LoRA target parameters enumerated (0): all linear layers
2025-11-02 05:45:23,925 | INFO | train_grpo | Trainable LoRA parameters (192): ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight shape=(4096, 4)', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight shape=(512, 4)', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight shape=(512, 4)', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight shape=(4, 4096)', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight shape=(2880, 4)', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight shape=(4096, 4)', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight shape=(512, 4)', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight shape=(512, 4)', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight shape=(4, 4096)', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight shape=(2880, 4)', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight shape=(4096, 4)', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight shape=(512, 4)'] ... (+172 more)
2025-11-02 05:45:24,226 | INFO | train_grpo | StepStream configured | prompts_per_micro_step=1 | num_generations=4 | dataset_rows=30 | keep_keys=['reward_action_0', 'reward_action_1', 'reward_action_2', 'reward_action_3', 'prompt']
2025-11-02 05:45:24,273 | INFO | train_grpo | Generation config | num_generations=4 | generation_batch_size=4 | per_device_train_batch_size=4 | grad_accum=4 | split_batches=True | completions_per_micro_step=4 | completions_per_update=16
2025-11-02 05:45:24,309 | INFO | train_grpo | Starting training | total_steps=10 | output_dir=runs/grpo_gptoss20b_lora4_tes
2025-11-02 05:54:44,151 | INFO | train_grpo | Loaded policy model dtype=torch.bfloat16
2025-11-02 05:54:44,151 | INFO | train_grpo | HF device map: {'': 0}
2025-11-02 05:54:44,268 | INFO | train_grpo | LoRA target parameters enumerated (0): all linear layers
2025-11-02 05:54:44,270 | INFO | train_grpo | Trainable LoRA parameters (192): ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight shape=(4096, 4)', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight shape=(512, 4)', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight shape=(512, 4)', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight shape=(4, 4096)', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight shape=(2880, 4)', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight shape=(4096, 4)', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight shape=(512, 4)', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight shape=(512, 4)', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight shape=(4, 4096)', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight shape=(2880, 4)', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight shape=(4096, 4)', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight shape=(4, 2880)', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight shape=(512, 4)'] ... (+172 more)
2025-11-02 05:54:44,557 | INFO | train_grpo | StepStream configured | prompts_per_micro_step=1 | num_generations=4 | dataset_rows=30 | keep_keys=['reward_action_0', 'reward_action_1', 'reward_action_2', 'reward_action_3', 'prompt']
2025-11-02 05:54:44,604 | INFO | train_grpo | Generation config | num_generations=4 | generation_batch_size=4 | per_device_train_batch_size=4 | grad_accum=4 | split_batches=True | completions_per_micro_step=4 | completions_per_update=16
2025-11-02 05:54:44,642 | INFO | train_grpo | Starting training | total_steps=10 | output_dir=runs/grpo_gptoss20b_lora4_tes
